<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Landmarks</title>

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/night.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
					<section data-markdown>
							<textarea data-template>
							### Landmarks

							##### Saad and Beau
							</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
								### Overview from last time

								- Split into two landmarks
								- Pantheon
								- Rialto Bridge
								<aside class="notes">
						SAAD

						Pretty simple. Only two landmarks. Only a couple convolutional layers.  </aside>	
								</textarea>
							</section>
							<section data-markdown>
								<textarea data-template>
								### Previous Architecture

								![](image/pre.png)
								<aside class="notes">
						SAAD </aside>	
								</textarea>
						</section>
						<section data-markdown>
							<textarea data-template>
							### Outcome

							 - Accuracy: 0.80077355
							 - Loss: 0.438
							 - Global Step:  337
							 - Accuracy is still increasing per step

							 <aside class="notes">
						SAAD

						Still in initial stages but 80 percent for two is bad.  </aside>	

							</textarea>
					</section>
						<section data-transition="slide" data-markdown data-background = "red">
								<textarea data-template>
							### So, what's new?
								</textarea>
						</section>
						<section data-markdown>
							<textarea data-template>
							### What's new?

							- New landmarks
							- New architecture (transfer learning - mobilenet and InceptionV3)
							- Data augmentation (brightness, scale)
							- Adjusting learning rate and batch size
							- Comparing results

							<aside class="notes">
						BEAU  </aside>	

							</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
						### New Landmarks

						- Subsampled 20,000 images of landmarks. These were the top 5:

						<aside class="notes">
						BEAU

						Our approach rather than to train on all of it, was to subsample a portion of the data and take the top landmarks from that sample (assuming proportional to population) and use transfer learning approach because it requires less data. </aside>	

						</textarea>
				</section>
						<section data-markdown>
								<textarea data-template>
						Wallis Tower

						![](image/wallis.jpg)
						
								</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
						Colosseum

						![](image/col.jpg)

								</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
						St. Pete's Basilica

						![](image/basilica.jpg)

								</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
						Charles Bridge

						![](image/bridge.jpg)

								</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
						Alhambra Palace

						![](image/ala.jpg)

								</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
								### New Architecture
								- MobileNet
								- InceptionV3

								<aside class="notes">
						SAAD  </aside>	
								</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
								### MobileNet

								<aside class="notes">
						BEAU The first net we are going to discuss. </aside>
								
								</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
								### MobileNet
								- Uses depthwise separable convolutions
								- Depthwise separable convolutions similar to regular convolutions. 
								- Requires less computation and is more efficient

								<aside class="notes">
						BEAU  </aside>	

								</textarea>
						</section>

						<section data-markdown>
								<textarea data-template>
							### Regular Convolution

							![](image/regular.png)

							<aside class="notes">
						SAAD A regular convolutional layer applies a convolution kernel (or “filter”) to all of the channels of the input image. It slides this kernel across the image and at each step performs a weighted sum of the input pixels covered by the kernel across all input channels. the convolution writes a new output pixel with only a single channel.  </aside>		
									</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
							### Depth-wise Separable Convolution

							- Depth-wise convolution
							![](image/depth1.PNG)
							<aside class="notes">
						SAAD Unlike a regular convolution it does not combine the input channels but it performs convolution on each channel separately. For an image with 3 channels, a depthwise convolution creates an output image that also has 3 channels. Each channel gets its own set of weights.  </aside>	
									</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
							### Depth-wise Separable Convolution

							- Point-wise convolution
							![](image/depth2.PNG)
							<aside class="notes">
						SAAD In other words, this simply adds up all the channels (as a weighted sum). As with a regular convolution, we usually stack together many of these pointwise kernels to create an output image with many channels. The purpose of this pointwise convolution is to combine the output channels of the depthwise convolution to create new features.  </aside> 
									</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
									### Add those two together to get depth-wise separable convolutions. 
									The end results of both approaches are pretty similar — they both filter the data and make new features — but a regular convolution has to do much more computational work to get there and needs to learn more weights.
									<aside class="notes">
						SAAD  </aside>	
								</textarea>
						</section>

						<section data-markdown>
								<textarea data-template>
									![](image/regconv.PNG)
									![](image/depthconv.PNG)
									<aside class="notes">
						BEAU The main idea is that the addition between two steps yields less than the multiplication in the regular convolution.  </aside> 
								</textarea>
						</section>

						<section data-markdown>
								<textarea data-template>
									![](image/depthconv2.PNG)
									<aside class="notes">
						SAAD Here, we can clearly see that the regular convolution performs about 9X worse than the depth-separable convolution.  </aside> 
								</textarea>
						</section>

						<section data-markdown>
								<textarea data-template>

									### MobileNet Network

									The full MobileNets network has 30 layers. The design of the network is quite straightforward:

									![](image/mobilenet.png)

									<aside class="notes">
						BEAU  </aside>	

						</textarea> </section>

						<section data-markdown>
								<textarea data-template>
									<ol>
										<li> 3X3 convolution as first layer, followed by 13 times shown image. </li>
										<li> Typically, average pooling layer at very end followed by FC classification layer (or 1X1 conv) and softmax </li>
									</ol>

									<aside class="notes">
						BEAU  </aside>	
								</textarea>
						</section>

						<section data-markdown>
								<textarea data-template>
									### More MobileNet
									- No pooling layers in between depth-wise blocks
									- Instead, some depthwise layers have stride 2
									- Reduces spatial dimensions. Pointwise layers, then increases output channels. 
									- Convolution layers followed by batch norm
									- Uses RELU activation. 
									- Using depthwise separable, mobilenet does 9X less work than comparable neural nets (w/ same accuracy).
									- Total # of parameters: 4,221,032. 

									<aside class="notes">
						SAAD (BEAU STRIDES)  </aside>	
								</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
									### MobileNet Results
									- Retrained MobileNet with our images (transfer learning) 
									- Final Test Accuracy (after 1000 steps): 89.4%

									<aside class="notes">
						SAAD  </aside>	
								</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
									![](image/accuracy.png)
									<aside class="notes">
						BEAU  </aside>	
								</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
									![](image/loss_mobile.png)
									<aside class="notes">
						BEAU  </aside>	
								</textarea>
						</section>

						<section data-markdown>
					<textarea data-template>

					### Data Augmentation

					- Flip_left_right: Randomly flip half of the training images horizontally.
					- Random_scale: Randomly scaled up by 10%
					- Random brightness: Randomly multiplyed input pixels up and down by 5%.
					- Random_crop: Randomly crop off 5% of images. 
					- Effectively creates "new" data from distortions
					- The biggest disadvantage is that the bottleneck caching is no longer useful, since input images are never reused exactly. 
					

					<aside class="notes">
						BEAU  </aside>	
					
					</textarea>
			</section>

			<section data-markdown>
					<textarea data-template>

					### Data Augmentation Results

					- Accuracy: 90.8%

					![](image/augmentation.png)

					<aside class="notes">
						BEAU  </aside>	

					
					</textarea>
			</section>


						<section data-markdown>
								<textarea data-template>
								### InceptionV3
								<aside class = "notes">
					SAAD MobileNet optimized for speed, but wanted to try InceptionV3 for better accuracy.  </aside>
								</textarea>
						</section>
						<section data-markdown>
							<textarea data-template>
							### It's a more complex model

							- What is an inception module?
							- Acts as multiple convolution filters at the same time, applied to the same input (then concatenated)
							- Multi-level feature extraction
							- InceptionV3 is a collection of stacked inception modules. 
							<aside class="notes">
						SAAD  </aside>	
							</textarea>
					</section>	
					<section data-markdown>
						<textarea data-template>
						![](image/naive.png)
						<aside class="notes">
						SAAD Dimensions are increasing with naïve module. Lots of computation. Based on channels of input (256). </aside>
						</textarea>
						
				</section>	
				<section data-markdown>
					<textarea data-template>
					![](image/naive2.png)
					<aside class="notes">
					BEAU </aside>	
					</textarea>
			</section>		
			<section data-markdown>
					<textarea data-template>
					![](image/naive3.png)
					<aside class = "notes">
					BEAU Adds 1X1 convolutions, reduced dimensions of your input which in turn leads to less computation. Also leads to less dimensions (channels) in output. </aside>
					</textarea>
					
			</section>	<section data-markdown>
					<textarea data-template>
					![](image/naive4.png)
					<aside class="notes">
					BEAU  </aside>	
					</textarea>
			</section>	

			<section data-markdown>
					<textarea data-template>
					### Reminder: Regular Convolution with one difference


					![](image/regular.png)
					<aside class="notes">
		SAAD As shown in 1X1 convolution, pertains to output channels of previous layer rather than RGB. 
	</aside>
					</textarea>
			</section>

			<section data-markdown>
					<textarea data-template>
					![](image/inception_model.png)
					<aside class="notes">
		SAAD Normal network: convolution, pooling, normalization, convolutional 2X, normalization, pooling

		BEAU MODULES
	</aside>
					</textarea>
			</section>	

			<section data-markdown>
					<textarea data-template>
					![](image/inception_model2.png)

					<aside class="notes">
		SAAD	Final network: Concatenate, pooling, convolutional/FC, softmax

		BEAU BACK PROP
	</aside>
					</textarea>
			</section>

			<section data-markdown>
					<textarea data-template>
					### Results

					- For 1000 steps, get accuracy of: 94%
					![](image/inception2.png)
					![](image/inception1.png)
					<aside class="notes">
						SAAD  </aside>	
					</textarea>
			</section>			
			<section data-markdown>
					<textarea data-template>
						### Comparison
						We have results for MobileNet and InceptionV3, but which one should we use?

						![](image/comparison.PNG)

						<aside class="notes">
						BEAU  </aside>	

					</textarea>
			</section>

			<section data-markdown>
				<textarea data-template>
				![](image/time.png)


				<aside class="notes">
						Pretty simple. Only two landmarks. Only a couple convolutional layers.  </aside>	
			</textarea>
			</section>


			<section data-markdown>
					<textarea data-template>

					- After seeing 1000 steps, training accuracy was still increasing. 
					- Wanted to try more steps. 
					- With 4000 steps, train accuracy got to 100%.
					- Test accuracy for to 94.9%. 
					- Try lower learning rate? See if accuracy increases. 

					<aside class="notes">
						SAAD  </aside>	
					
					</textarea>
			</section>

			<section data-markdown>
					<textarea data-template>

					Trying learning rate = 0.005

					- Test accuracy = 95.4%

					![](image/batch_reborn.PNG)
					![](image/learning_rate2.PNG)

					<aside class="notes">
						SAAD  </aside>	

					
					</textarea>
			</section>

			<section data-markdown>
					<textarea data-template>

					### More Data

					- Will adding more data improve our validation accuracy? 
					- We subsampled for 200,000 images of landmarks, then filtered for the five previous landmarks used. 
					- We then retrained InceptionV3 using this added data.

					<aside class="notes">
						SAAD  </aside>	
					
					</textarea>
			</section>

			<section data-markdown>
					<textarea data-template>

					### More Results

					- We now are running with 4,000 steps on our new data.
					- It took forever to create the bottleneck files. 
					- It took forever to train the data.

					<aside class="notes">
						SAAD  </aside>	
					
					</textarea>
			</section>

			<section data-markdown>
					<textarea data-template>

					![](image/bigdata1.png)
					![](image/bigdata2.png)

					<aside class="notes">
						SAAD Training was kinda all over the place - jumping up and down. We want to implement some learning decay to help stop that.   </aside> 
					
					</textarea>
			</section>

			<section data-markdown>
					<textarea data-template>

					### Batch Size

					![](image/batch.png)

					- "Decaying the learning rate and increasing the batch size during training are equivalent" (according to these dudes)
					- Let's test this with a batch size of 300. 

					<aside class="notes">
						BEAU  </aside>	
					</textarea>
			</section>

			<section data-markdown>
					<textarea data-template>

					### Batch Size Results

					- Accuracy: 92% :(
					- But, we did see less jumping around during training. 
					
					<aside class="notes">
						BEAU  </aside>	
					</textarea>
			</section>

			<section data-markdown>
				<textarea data-template>
				![](image/batch_fail.png)
				![](image/bigdata1.png)

				<aside class="notes">
						BEAU  </aside>	
			</textarea>
			</section>


<section data-markdown>
					<textarea data-template>

					### Misclassification Examples

					- Misclassified as: St. Peter's Basilica

					![](image/col1.jpg)
					![](image/lastone.png)

					<aside class="notes">
						SAAD </aside>	

					
					</textarea>
			</section>

			<section data-markdown>
					<textarea data-template>

					### Misclassification Examples

					- Misclassified as: St. Peter's Basilica

					![](image/col2.jpg)
					![](image/stpeteresult.png)

					<aside class="notes">
						SAAD </aside>	

					
					</textarea>
			</section>

<section data-markdown>
					<textarea data-template>

					### Misclassification Examples

					- Misclassified as: Charles Bridge

					![](image/col3.jpg)
					![](image/charles_results.png)

					<aside class="notes">
						SAAD </aside>	

					
					</textarea>
			</section>


			

			<section data-markdown>
					<textarea data-template>

					### Takeaways

					- Amount of time taken: Inception was longer. 
					- Accuracy vs Speed
					- MobileNet training seemed went to 100% faster (test wasn't as good)
					- Lower learning rate and larger batch size ideally would give smoother accuracy curve. 
					- Data Augmentation seemed to smooth it as well (on MobileNet)

					<aside class="notes">
						SAAD  </aside>	
					
					</textarea>
			</section>

			<section data-markdown>
					<textarea data-template>

					### Takeaways

					![](image/summary.PNG)

					<aside class="notes">
						SAAD  </aside>	
					
					</textarea>
			</section>







			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
	</body>
</html>
